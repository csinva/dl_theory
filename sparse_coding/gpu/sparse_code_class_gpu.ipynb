{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from os.path import join as oj\n",
    "import sys\n",
    "sys.path.append('../vision_fit')\n",
    "sys.path.append('../vision_analyze')\n",
    "import viz_weights\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import pickle as pkl\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise\n",
    "import matplotlib.pyplot as plt\n",
    "import models\n",
    "from dim_reduction import *\n",
    "from sklearn.decomposition import MiniBatchDictionaryLearning\n",
    "from tqdm import tqdm\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "(1000, 3072)\n"
     ]
    }
   ],
   "source": [
    "# load dset\n",
    "root = oj('/scratch/users/vision/yu_dl/raaz.rsk/data', 'cifar10')\n",
    "trans = transforms.Compose([transforms.ToTensor()])\n",
    "test_set = dset.CIFAR10(root=root, train=False, download=True)\n",
    "X_test = test_set.test_data\n",
    "Y_test = np.array(test_set.test_labels)\n",
    "lab_dict = {0: 'airplane', 1: 'automobile', 2: 'bird', 3: 'cat', 4: 'deer', 5: 'dog', 6: 'frog', 7: 'horse', 8: 'ship', 9: 'truck'}\n",
    "\n",
    "# filter by a class\n",
    "idxs = Y_test==1\n",
    "X = X_test[idxs]\n",
    "Y = Y_test[idxs]\n",
    "\n",
    "# look at an image or 2\n",
    "# num = 0\n",
    "# plt.imshow(X[num], interpolation=None)\n",
    "# plt.title(lab_dict[Y[num]])\n",
    "# plt.show()\n",
    "\n",
    "X_d = X.reshape(X.shape[0], -1)\n",
    "print(X_d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = 1.0\n",
    "# n_iter = 1\n",
    "# n_components = 256\n",
    "# batch_size = 100\n",
    "\n",
    "# dico = MiniBatchDictionaryLearning(n_components=n_components, alpha=alpha, n_iter=n_iter, n_jobs=1, batch_size=batch_size) \n",
    "# for i in tqdm(range(50000)):\n",
    "#     V = dico.fit(X_d)\n",
    "#     if i % 100 == 0:\n",
    "#         np.save('sparse_cifar/bases_iters=' + str(i) + '_alpha=' + str(alpha) + '_ncomps=' + str(n_components) + '.npy', V.components_)        \n",
    "# #         viz_weights.plot_weights(V.components_, dset='cifar10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viz_weights.plot_weights(V.components_, dset='rgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get gpu code to work..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('ke_sparse_coding_pytorch/EE290T_quantized_sparse_codes')\n",
    "from ke_sparse_coding_pytorch.EE290T_quantized_sparse_codes.training import sparse_coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6036f0bffb0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_bases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbases_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbases_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbases_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'sparsity_weight'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'max_num_iters'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "num_bases = 10\n",
    "X_t = torch.Tensor(X.reshape(1000, -1, 1)).cuda()\n",
    "bases_init = np.random.uniform(size=(1000, num_bases))\n",
    "bases_init = torch.Tensor(bases_init)\n",
    "d = {'sparsity_weight': 1, 'max_num_iters': 1}\n",
    "sched = {x:d for x in range(4)}\n",
    "sparse_coding.train_dictionary(X_t, bases_init, all_params={'num_epochs': 1, 'code_inference_algorithm': 'ista', 'dictionary_update_algorithm': 'sc_steepest_descent', \n",
    "                                                          'inference_param_schedule': sched, 'dict_update_param_schedule': sched})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train a sparse coding dictionary. These settings for Field natural images dset\n",
    "\"\"\"\n",
    "import sys\n",
    "import os\n",
    "# examples_fullpath = os.path.dirname(os.path.abspath(__file__))\n",
    "# toplevel_dir_fullpath = examples_fullpath[:examples_fullpath.rfind('/')+1]\n",
    "# sys.path.insert(0, toplevel_dir_fullpath)\n",
    "sys.path.append('ke_sparse_coding_pytorch/EE290T_quantized_sparse_codes')\n",
    "\n",
    "import argparse\n",
    "import pickle\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "\n",
    "from training.sparse_coding import train_dictionary as sc_train\n",
    "from utils.plotting import TrainingLivePlot\n",
    "from utils.image_processing import create_patch_training_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_patches_gpu = torch.Tensor(X_d[:10]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_IDENTIFIER = 'test_sparse_coding'\n",
    "\n",
    "BATCH_SIZE = 250\n",
    "NUM_BATCHES = 4000  # 1 million patches total\n",
    "PATCH_HEIGHT = 16\n",
    "PATCH_WIDTH = 16\n",
    "\n",
    "CODE_SIZE = 256\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "SC_PARAMS = {\n",
    "    'num_epochs': NUM_EPOCHS,\n",
    "    'code_inference_algorithm': 'fista',\n",
    "    'inference_param_schedule': {\n",
    "      0: {'sparsity_weight': 0.1, 'max_num_iters': 50},\n",
    "      10*NUM_BATCHES: {'sparsity_weight': 0.1, 'max_num_iters': 100},\n",
    "      20*NUM_BATCHES: {'sparsity_weight': 0.1, 'max_num_iters': 200}},\n",
    "    'dictionary_update_algorithm': 'sc_cheap_quadratic_descent',\n",
    "    'dict_update_param_schedule': {\n",
    "      0: {'stepsize': 0.05, 'num_iters': 1},\n",
    "      10*NUM_BATCHES: {'stepsize': 0.01, 'num_iters': 1},\n",
    "      20*NUM_BATCHES: {'stepsize': 0.005, 'num_iters': 1}},\n",
    "    'training_visualization_schedule': {0: None, 1000: None, 2000: None}}\n",
    "SC_PARAMS['training_visualization_schedule'].update(\n",
    "    {NUM_BATCHES*x: None for x in range(NUM_EPOCHS)})\n",
    "\n",
    "# Arguments for dataset and logging\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"data_id\",\n",
    "    help=\"Name of the dataset (currently allowable: \" +\n",
    "         \"Field_NW_whitened, Field_NW_unwhitened)\")\n",
    "parser.add_argument(\"data_filepath\", help=\"The full path to dataset on disk\")\n",
    "parser.add_argument(\"-l\", \"--logfile_dir\",\n",
    "                    help=\"Optionally checkpoint the model here\")\n",
    "# script_args = parser.parse_args()\n",
    "\n",
    "# if script_args.logfile_dir is not None:\n",
    "SC_PARAMS['checkpoint_schedule'] = {'checkpoint_folder_fullpath': RUN_IDENTIFIER,\n",
    "      NUM_BATCHES: None, 10*NUM_BATCHES: None, 20*NUM_BATCHES:None}\n",
    "\n",
    "# torch_device = torch.device('cuda:1')\n",
    "# torch.cuda.set_device(1)\n",
    "# otherwise can put on 'cuda:0' or 'cpu'\n",
    "\n",
    "# manually create large training set with one million whitened patches\n",
    "'''\n",
    "one_mil_image_patches = create_patch_training_set(\n",
    "    ['patch'], (PATCH_HEIGHT, PATCH_WIDTH), BATCH_SIZE, NUM_BATCHES,\n",
    "    edge_buffer=5, dataset=script_args.data_id,\n",
    "    datasetparams={'filepath': script_args.data_filepath,\n",
    "                   'exclude': []})['batched_patches']\n",
    "'''\n",
    "\n",
    "#################################################################\n",
    "# save these to disk if you want always train on the same patches\n",
    "# or if you want to speed things up in the future\n",
    "#################################################################\n",
    "# pickle.dump(one_mil_image_patches, open('/media/expansion1/spencerkent/Datasets/Field_natural_images/one_million_patches_October24.p', 'wb'))\n",
    "\n",
    "# one_mil_image_patches = pickle.load(open(\n",
    "#     '/media/expansion1/spencerkent/Datasets/Field_natural_images/one_million_patches_October24.p', 'rb')).astype('float32')\n",
    "\n",
    "# send ALL image patches to the GPU\n",
    "# image_patches_gpu = torch.from_numpy(one_mil_image_patches).to(torch_device)\n",
    "\n",
    "# create the dictionary Tensor on the GPU\n",
    "sparse_coding_dictionary = torch.randn((PATCH_HEIGHT*PATCH_WIDTH, CODE_SIZE))\n",
    "sparse_coding_dictionary.div_(sparse_coding_dictionary.norm(p=2, dim=0))\n",
    "\n",
    "# Create the LivePlot object\n",
    "liveplot_obj = TrainingLivePlot(\n",
    "    dict_plot_params={'total_num': CODE_SIZE, 'img_height': PATCH_HEIGHT,\n",
    "                      'img_width': PATCH_WIDTH, 'plot_width': 16,\n",
    "                      'plot_height': 16, 'renorm imgs': True,\n",
    "                      'display_ordered': True},\n",
    "    code_plot_params={'size': CODE_SIZE})\n",
    "\n",
    "SC_PARAMS['training_visualization_schedule']['liveplot_object_reference'] = liveplot_obj\n",
    "\n",
    "print(\"Here we go!\")\n",
    "sc_train(image_patches_gpu, sparse_coding_dictionary, SC_PARAMS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
