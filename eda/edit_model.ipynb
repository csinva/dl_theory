{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from os.path import join as oj\n",
    "import sys\n",
    "sys.path.append('../vision_fit')\n",
    "sys.path.append('../vision_analyze')\n",
    "import data\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import pickle as pkl\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise\n",
    "import matplotlib.pyplot as plt\n",
    "import models\n",
    "from dim_reduction import *\n",
    "import viz_weights\n",
    "import siamese\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# load some models             \n",
    "convnet = models.LeNet()\n",
    "linnet = models.LinearNet(4, 28*28, 256, 10)\n",
    "model = linnet\n",
    "model = model.cuda()\n",
    "\n",
    "# load mnist\n",
    "from params_vision import p\n",
    "p.dset = 'mnist'\n",
    "p.shuffle_labels = False\n",
    "train_loader, test_loader = data.get_data_loaders(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with reps\n",
    "linnet = models.LinearNet(4, 28*28, 256, 10).cuda()\n",
    "lenet = models.LeNet().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = oj('/scratch/users/vision/yu_dl/raaz.rsk/data/mnist')\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "train_set = dset.MNIST(root=root, train=True, transform=trans, download=True)\n",
    "test_set = dset.MNIST(root=root, train=False, transform=trans, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get the full dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, Y_train, X_test, Y_test = data.process_loaders(train_loader, test_loader)\n",
    "X_train, Y_train_onehot = data.get_XY(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# siamese net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = siamese.SiameseNet(lenet, X_train, Y_train_onehot, \n",
    "                           reps=2, similarity='dot', siamese_init='unif', \n",
    "                           train_prototypes=True, prototype_dim=14).cuda()\n",
    "x = torch.Tensor(X_train[0:2]).cuda()\n",
    "y = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data-driven init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init final lay ws\n"
     ]
    }
   ],
   "source": [
    "from vision_fit.init import *\n",
    "\n",
    "# final layer only\n",
    "# initializes final ws = xs (with appropriate class)\n",
    "# intializes bs = 0 \n",
    "# X should have atleast as many examples as there are weights in any layer\n",
    "def initialize_ws_and_zero_bias_lay_final(X, Y_train_onehot, model):\n",
    "    print('init final lay ws')\n",
    "    \n",
    "    # pick the examples on the first iteration\n",
    "    # get prototype images for each label (reps is how many repeats)\n",
    "    # returns images (X) and labels (Y)\n",
    "    def get_ims_per_lab(X_train, Y_train_onehot, reps=1):\n",
    "        exs = np.zeros((10 * reps, X_train.shape[1]))\n",
    "        labs = np.zeros(10 * reps)\n",
    "        for i in range(10):\n",
    "            idxs = Y_train_onehot[:, i] == 1\n",
    "            exs[reps * i: reps * (i + 1)] = X_train[idxs][:reps]\n",
    "            labs[reps * i: reps * (i + 1)] = i\n",
    "        return exs, labs\n",
    "\n",
    "    exs, _ = get_ims_per_lab(X_train, Y_train_onehot, p.reps)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        exs = torch.Tensor(exs).cuda()\n",
    "    else:\n",
    "        exs = torch.Tensor(exs)\n",
    "    \n",
    "        \n",
    "    acts = model.features(exs)\n",
    "    # preserve model layer norm\n",
    "    model.last_lay().weight.data = acts / acts.norm() * model.last_lay().weight.data.norm()\n",
    "\n",
    "\n",
    "initialize_ws_and_zero_bias_lay_final(X_train, Y_train_onehot, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "exs, _ = get_ims_per_lab(X_train, Y_train_onehot, p.reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 784)\n"
     ]
    }
   ],
   "source": [
    "exs2 = np.mean(exs, axis=0).reshape(1, -1)\n",
    "print(exs2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "viz_weights.plot_weights(exs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
