{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes torch.Size([500, 784]) (500, 784)\n",
      "shapes torch.Size([256, 500]) (256, 500)\n",
      "shapes torch.Size([10, 256]) (10, 256)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from os.path import join as oj\n",
    "import sys\n",
    "sys.path.append('../vision_fit')\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import pickle as pkl\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise\n",
    "import matplotlib.pyplot as plt\n",
    "import models\n",
    "from dim_reduction import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# reduce model by projecting onto pcs that explain \"percent_to_explain\"\n",
    "def reduce_model(model, percent_to_explain=0.85):\n",
    "    model_r = deepcopy(model)\n",
    "    weight_dict = model_r.state_dict()\n",
    "    weight_dict_new = deepcopy(model_r.state_dict())\n",
    "#     print(weight_dict)\n",
    "    for layer_name in weight_dict.keys():\n",
    "        if 'weight' in layer_name:\n",
    "            w = weight_dict[layer_name]\n",
    "            \n",
    "            # get number of components\n",
    "            pca = PCA(n_components=w.shape[1])\n",
    "            pca.fit(w)\n",
    "            explained_vars = pca.explained_variance_ratio_\n",
    "            dim, perc_explained = 0, 0\n",
    "            while perc_explained <= percent_to_explain:\n",
    "                perc_explained += explained_vars[dim]\n",
    "                dim += 1\n",
    "            \n",
    "            # actually project\n",
    "            pca = PCA(n_components=dim)            \n",
    "            w2 = pca.inverse_transform(pca.fit_transform(w))\n",
    "            print('shapes', w.shape, w2.shape)\n",
    "            weight_dict_new[layer_name] = torch.Tensor(w2)\n",
    "            \n",
    "    model_r.load_state_dict(weight_dict_new)\n",
    "    return model_r\n",
    "\n",
    "             \n",
    "modelm = models.MnistNet()        \n",
    "modelr = reduce_model(modelm)\n",
    "convnet = models.LeNet()\n",
    "linnet = models.LinearNet(28*28, 4, 256, 10)\n",
    "ms = [modelm, modelr, convnet, linnet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.optim.sgd.SGD at 0x7f5db122f940>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for m in ms:\n",
    "#     print(models.get_weight_names(m))\n",
    "optim.SGD([x[1] for x in linnet.named_parameters()], lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['features.6.bias', 'features.6.weight', 'features.0.weight', 'features.10.weight', 'features.3.weight', 'features.10.bias', 'classifier.1.weight', 'classifier.6.weight', 'classifier.1.bias', 'classifier.4.weight', 'features.8.weight', 'features.8.bias', 'features.0.bias', 'classifier.4.bias', 'classifier.6.bias', 'features.3.bias']) odict_keys(['features.0.weight', 'features.0.bias', 'features.3.weight', 'features.3.bias', 'features.6.weight', 'features.6.bias', 'features.8.weight', 'features.8.bias', 'features.10.weight', 'features.10.bias', 'classifier.1.weight', 'classifier.1.bias', 'classifier.4.weight', 'classifier.4.bias', 'classifier.6.weight', 'classifier.6.bias'])\n"
     ]
    }
   ],
   "source": [
    "weight_dict = deepcopy({x[0]:x[1].data.cpu().numpy() for x in a.named_parameters()})\n",
    "print(weight_dict.keys(), a.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc.0.weight\n",
      "conv1.weight\n"
     ]
    }
   ],
   "source": [
    "# print(get_first_weight(modelm))\n",
    "print(models.get_weight_names(m2)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from params_vision import p\n",
    "np.random.seed(p.seed) \n",
    "torch.manual_seed(p.seed)    \n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "batch_size = 100\n",
    "root = oj('/scratch/users/vision/yu_dl/raaz.rsk/data', p.dset)\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)\n",
    "\n",
    "\n",
    "## load mnist dataset\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "train_set = dset.MNIST(root=root, train=True, transform=trans, download=True)\n",
    "test_set = dset.MNIST(root=root, train=False, transform=trans, download=True)\n",
    "\n",
    "\n",
    "ex_nums = {}\n",
    "i = 0\n",
    "while(len(ex_nums) < 10):\n",
    "    ex_nums[train_set.train_labels[i]] = i\n",
    "    i += 1\n",
    "    \n",
    "exs = np.zeros((10, 28, 28))\n",
    "for i in range(10):\n",
    "    exs[i] = train_set.train_data[i]\n",
    "    \n",
    "# train_set.train_data = torch.Tensor(exs)\n",
    "# train_set.train_labels = torch.Tensor(np.arange(0, 10)).long()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)\n",
    "model = models.MnistNet()  \n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_set.train_labels.size()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.array(train_set.train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.train_data[:100].shape\n",
    "train_set.train_labels[:100].shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    Y_big[:, i] = np.array(Y_train==i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Y_big/np.sum(Y_big, axis=0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.train_labels = torch.Tensor(np.random.randint(0, 10, 60000)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 8, 4, ..., 6, 0, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0, 10, 60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "test_set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "root = oj('/scratch/users/vision/yu_dl/raaz.rsk/data', 'cifar10')\n",
    "train_set = dset.CIFAR10(root=root, train=True, download=True, transform=trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set.train_labels), type(train_set.train_labels[0])\n",
    "train_set.train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<filter object at 0x7f537666c240>\n"
     ]
    }
   ],
   "source": [
    "print(filter(lambda p: p.requires_grad, m.parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, modelm.parameters()), lr=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "for param_group in optimizer.param_groups:\n",
    "        print(param_group['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
